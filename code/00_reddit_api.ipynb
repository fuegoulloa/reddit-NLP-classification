{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae561564-3c91-4a6d-85d8-f71dc0231a6f",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"../images/ga_logo_large.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1606000-b468-49e5-977b-42fab9891309",
   "metadata": {},
   "source": [
    "---\n",
    "## **Project 3: Web APIs and NLP**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeb4c70-7d3b-4a99-871c-901aae0ab890",
   "metadata": {},
   "source": [
    "---\n",
    "**Script to use Reddit API and scrape data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36058e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import getpass\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f773189e-c49b-4bb1-ba9c-f361ceec043f",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "**Access Info**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee811348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n",
      " ········\n",
      " ········\n",
      " ········\n",
      " ········\n"
     ]
    }
   ],
   "source": [
    "client_id = getpass.getpass()       # alphanumeric string provided under \"personal use script\"\n",
    "client_secret = getpass.getpass()   # alphanumeric string provided as \"secret\"\n",
    "user_agent = getpass.getpass()      # name of application\n",
    "username = getpass.getpass()        # reddit username\n",
    "password =  getpass.getpass()       # reddit password"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda5cd46-b80f-4cb5-ae7f-80c6fdefb1ff",
   "metadata": {},
   "source": [
    "**Parsing Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22609926-f23e-4c6f-82f6-81520fef5d33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_data(response, subreddit):\n",
    "    \n",
    "    batch = response.json()['data']['children']\n",
    "\n",
    "    data = []\n",
    "    output_path = '../data/reddit--' + subreddit + '.csv'\n",
    "\n",
    "    for item in batch:\n",
    "        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        post = {\n",
    "            'post_id' : item['data']['name'],\n",
    "            'post_title' : item['data']['title'],\n",
    "            'post_text' : item['data']['selftext'],\n",
    "            'published_on': item['data']['created_utc'],\n",
    "            'scraped_on': timestamp \n",
    "        }\n",
    "        data.append(post)\n",
    "        \n",
    "# **===== consulted chatgpt for some of the lines on this specific bloc ===================**\n",
    "    # check if there are posts to proces\n",
    "    if not data:\n",
    "        print(\"No new posts to process, try later\")\n",
    "        return None, pd.DataFrame()\n",
    "# **=======================================================================================**\n",
    "\n",
    "    # build df\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # check if file already exists to write header\n",
    "    if not os.path.isfile(output_path): # <<<---- chatgpt help for this logic\n",
    "        # write header if file does not exist\n",
    "        df.to_csv(output_path, index = False)\n",
    "    else:\n",
    "        # append to existing file, omit header\n",
    "        df.to_csv(output_path, mode = 'a', header = False, index = False)\n",
    "    \n",
    "    # fetch last post id\n",
    "    last_id = data[-1]['post_id']\n",
    "    \n",
    "    return last_id, df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e990710-0f91-4e54-aaa4-22d47089c085",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f68529b-fb5a-42be-97dc-e6d84b5dff6e",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## **Full Script Scraping Script Here**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40c456a-df3f-4024-8250-ae691340d59c",
   "metadata": {},
   "source": [
    "Run this cell to scrape data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "794a53a1-9dbd-43fc-9ae1-cf3c4a796c3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Established connection\n",
      "Success, token retrieved\n",
      "Scraping request number 1 for RealEstate subreddit in progress. Connection status: 200\n",
      "Scraping request number 2 for RealEstate subreddit in progress. Connection status: 200\n",
      "Scraping request number 3 for RealEstate subreddit in progress. Connection status: 200\n",
      "Scraping request number 4 for RealEstate subreddit in progress. Connection status: 200\n",
      "Scraping request number 5 for RealEstate subreddit in progress. Connection status: 200\n",
      "Scraping request number 6 for RealEstate subreddit in progress. Connection status: 200\n",
      "Scraping request number 7 for RealEstate subreddit in progress. Connection status: 200\n",
      "Scraping request number 8 for RealEstate subreddit in progress. Connection status: 200\n",
      "Scraping request number 9 for RealEstate subreddit in progress. Connection status: 200\n",
      "Scraping request number 10 for RealEstate subreddit in progress. Connection status: 200\n",
      "No new posts to process, try later\n",
      "No more posts available for subreddit: RealEstate\n",
      "Scraping request number 1 for travel subreddit in progress. Connection status: 200\n",
      "Scraping request number 2 for travel subreddit in progress. Connection status: 200\n",
      "Scraping request number 3 for travel subreddit in progress. Connection status: 200\n",
      "Scraping request number 4 for travel subreddit in progress. Connection status: 200\n",
      "Scraping request number 5 for travel subreddit in progress. Connection status: 200\n",
      "Scraping request number 6 for travel subreddit in progress. Connection status: 200\n",
      "Scraping request number 7 for travel subreddit in progress. Connection status: 200\n",
      "Scraping request number 8 for travel subreddit in progress. Connection status: 200\n",
      "Scraping request number 9 for travel subreddit in progress. Connection status: 200\n",
      "No new posts to process, try later\n",
      "No more posts available for subreddit: travel\n"
     ]
    }
   ],
   "source": [
    "# authorization request and initial connection-----------------------------------------\n",
    "\n",
    "#-----------  these lines are adapted from the API walkthrough ------------------------\n",
    "auth = requests.auth.HTTPBasicAuth(client_id, client_secret)\n",
    "\n",
    "data = {'grant_type': 'password',\n",
    "       'username': username,\n",
    "       'password': password}\n",
    "\n",
    "headers = {'User-Agent': 'dsb0826project3/0.0.1'}\n",
    "\n",
    "response = requests.post('https://www.reddit.com/api/v1/access_token',\n",
    "                        auth = auth,\n",
    "                        data = data,\n",
    "                        headers = headers)\n",
    "\n",
    "# response confirmation\n",
    "if response.status_code == 200:\n",
    "    print('Established connection')\n",
    "else:\n",
    "    print('Failed to connect, troubleshoot...')\n",
    "\n",
    "    \n",
    "# retrieve access token\n",
    "token = response.json()['access_token']\n",
    "headers['Authorization'] = f'bearer {token}'\n",
    "if requests.get('https://oauth.reddit.com/api/v1/me', headers = headers).status_code == 200:\n",
    "    print('Success, token retrieved')\n",
    "else:\n",
    "    print('Failed to retrieve token, troubleshoot...')\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# ----  API connection, data scraping and parsing --------------------------------------\n",
    "# ----  Iteratively improved, some lines adapted/debugged after consulting with chatgpt\n",
    "base_url = 'https://oauth.reddit.com/r/'\n",
    "subreddits = ['RealEstate', 'travel']\n",
    "\n",
    "\n",
    "    \n",
    "for sub in subreddits:\n",
    "    count = 1\n",
    "    last = None # reset this key for new subreddit\n",
    "    params = {'limit': 100, 'after': last}\n",
    "\n",
    "    while True: # scrape until no more posts are returned\n",
    "\n",
    "\n",
    "        response = requests.get(base_url + sub,\n",
    "                            headers = headers,\n",
    "                            params = params)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            print(f\"Scraping request number {count} for {sub} subreddit in progress. Connection status: {response.status_code}\")\n",
    "            last, df = parse_data(response, sub)\n",
    "\n",
    "            if last is None:\n",
    "                print(f\"No more posts available for subreddit: {sub}\")\n",
    "                break\n",
    "            # otherwise update the key to pass on to next request\n",
    "            params['after'] = last\n",
    "\n",
    "        else:\n",
    "            print(f\"Failed to scrape subreddit {sub}.  Status code: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "        # increase count variable\n",
    "        count += 1\n",
    "\n",
    "        # run WHILE loop iterations (scrapes) every 15 seconds\n",
    "        time.sleep(15)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
